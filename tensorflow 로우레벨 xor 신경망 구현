ë¡œìš°ë ˆë²¨ XOR ì‹ ê²½ë§ êµ¬í˜„ (1ì€ë‹‰ì¸µ, 1ì¶œë ¥ì¸µ, ì‹œê·¸ëª¨ì´ë“œ)
python
ë³µì‚¬
í¸ì§‘
import numpy as np

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ & ê·¸ ë„í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    return sigmoid(x) * (1 - sigmoid(x))

# ë°ì´í„° (XOR)
x_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])  # shape: (4, 2)
y_data = np.array([[0], [1], [1], [0]])     # shape: (4, 1)

# ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
W1 = np.random.randn(2, 2)   # (ì…ë ¥2 â†’ ì€ë‹‰2)
b1 = np.zeros((1, 2))        # ì€ë‹‰ì¸µ ë°”ì´ì–´ìŠ¤

W2 = np.random.randn(2, 1)   # (ì€ë‹‰2 â†’ ì¶œë ¥1)
b2 = np.zeros((1, 1))        # ì¶œë ¥ì¸µ ë°”ì´ì–´ìŠ¤

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
learning_rate = 0.1
epochs = 10000

# í•™ìŠµ ë°˜ë³µ
for epoch in range(epochs):
    # --- Forward ---
    z1 = np.dot(x_data, W1) + b1     # (4,2)
    a1 = sigmoid(z1)                 # ì€ë‹‰ì¸µ í™œì„±í™”ê°’

    z2 = np.dot(a1, W2) + b2         # (4,1)
    a2 = sigmoid(z2)                 # ì¶œë ¥ê°’

    # --- Loss (Binary Cross Entropy) ---
    loss = -np.mean(y_data * np.log(a2 + 1e-8) + (1 - y_data) * np.log(1 - a2 + 1e-8))

    # --- Backpropagation ---
    dz2 = a2 - y_data                    # (4,1)
    dW2 = np.dot(a1.T, dz2) / 4         # (2,1)
    db2 = np.sum(dz2, axis=0, keepdims=True) / 4

    dz1 = np.dot(dz2, W2.T) * sigmoid_deriv(z1)  # (4,2)
    dW1 = np.dot(x_data.T, dz1) / 4              # (2,2)
    db1 = np.sum(dz1, axis=0, keepdims=True) / 4

    # --- Gradient Descent ---
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1

    # --- ì¶œë ¥ ---
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# --- í•™ìŠµ í›„ ê²°ê³¼ ---
print("\nğŸ” ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼:")
a2_final = sigmoid(np.dot(sigmoid(np.dot(x_data, W1) + b1), W2) + b2)
print(np.round(a2_final, 4))
