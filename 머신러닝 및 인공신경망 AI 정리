[머신러닝의 정의 및 분류]

1. 머신러닝의 기본 개념
**머신러닝(Machine Learning)**은 1959년 Arthur Samuel이 정의한 바와 같이, "명시적으로 프로그래밍하지 않고도 컴퓨터가 학습할 수 있는 능력을 부여하는 연구 분야"입니다. 이 분야는 기존의 명시적 프로그래밍 방식이 가진 한계, 예를 들어 스팸 필터를 위한 수많은 규칙을 작성하거나 자율 주행을 위한 너무나 많은 규칙을 명시해야 하는 복잡성을 극복하기 위해 등장했습니다.
머신러닝은 크게 두 가지 주요 학습 유형으로 나뉩니다.
• 지도 학습 (Supervised Learning)
    ◦ 정의: 레이블이 지정된, 즉 정답이 있는 데이터를 사용하여 학습하는 방식입니다. 머신러닝에서 가장 흔하게 접할 수 있는 문제 유형입니다.
    ◦ 예시: 이미지 라벨링(태그가 지정된 이미지로부터 학습), 이메일 스팸 필터(스팸 또는 햄으로 분류된 이메일로부터 학습), 그리고 과거 시험 점수와 공부 시간을 바탕으로 미래 시험 점수를 예측하는 것 등이 있습니다.
    ◦ 세부 유형: 지도 학습은 예측하려는 값의 형태에 따라 다시 회귀와 분류로 나뉩니다.
        ▪ 회귀 (Regression): 연속적인 값을 예측하는 문제입니다. 예를 들어, 공부 시간에 따른 최종 시험 점수를 예측하는 것이 이에 해당합니다. 선형 회귀에서는 **"H(x) = Wx + b"**와 같은 **가설 함수(Hypothesis)**를 사용하여 입력 데이터와 출력 사이의 관계를 모델링합니다.
        ▪ 분류 (Classification): 이산적인 카테고리(범주)를 예측하는 문제입니다.
            • 이진 분류 (Binary Classification): 두 가지 카테고리 중 하나를 예측하는 경우입니다. 합격/불합격, 스팸/햄 등이 예시이며, 보통 결과를 0 또는 1로 인코딩합니다. 선형 회귀 모델은 예측값이 0보다 작거나 1보다 클 수 있으므로, 이러한 문제에는 **시그모이드(Sigmoid) 함수를 활성 함수로 사용하는 로지스틱 회귀(Logistic Regression)**가 사용됩니다. 로지스틱 회귀의 가설 함수는 "H(X) = 1 / (1 + e^-WTX)" 형태를 가집니다.
            • 다중 분류 (Multi-label Classification): 여러 카테고리 중 하나를 예측하는 경우입니다. 예를 들어, 공부 시간에 따른 성적 등급(A, B, C, D, F)을 예측하는 것이 이에 해당합니다. 다중 분류 문제에서는 주로 소프트맥스 회귀(Softmax Regression) 기법이 사용되며, 모델의 출력값에 Softmax 함수를 적용하여 각 레이블에 대한 확신의 정도를 확률로 출력합니다.
• 비지도 학습 (Unsupervised Learning)
    ◦ 정의: 레이블이 없는 데이터를 사용하여 학습하는 방식입니다.
    ◦ 예시: Google 뉴스 그룹화나 단어 클러스터링과 같이 데이터 내의 숨겨진 패턴이나 구조를 찾는 데 사용됩니다.

[머신러닝 모델 학습 과정 핵심 요소]

머신러닝 모델 학습 과정의 핵심 요소들은 다음과 같습니다:
1. 데이터 분할 (Data Splitting)
머신러닝 모델 학습의 첫 단계는 보유한 전체 데이터를 적절히 나누는 것입니다. 이는 모델이 **트레이닝 데이터(Training Data)**에 과도하게 최적화되는 오버피팅(Overfitting) 현상을 방지하고, 모델의 실제 성능을 공정하게 평가하기 위함입니다.
• 트레이닝 데이터: 모델의 최적의 파라미터를 찾기 위해 대량의 데이터와 충분한 시간을 들여 학습에 사용하는 데이터입니다.
• 검증 데이터(Validation Data): 트레이닝 과정에서 학습에 직접 사용되지는 않지만, 중간중간 모델을 테스트하여 오버피팅에 빠지지 않았는지 확인하는 데 사용됩니다. 즉, 트레이닝 과정 중간에 사용하는 테스트 데이터로 볼 수 있습니다.
• 테스트 데이터(Test Data): 트레이닝 과정에서 보지 못한 새로운 데이터에 모델을 적용하여 모델이 잘 학습되었는지 테스트하거나 실제 문제를 해결하는 데 사용됩니다. 모델의 학습 정도를 확인할 때 이를 "테스트(Test)"라고 하며, 실제 문제를 푸는 과정은 "추론(Inference)"이라고 부릅니다.
2. 가설 (Hypothesis)
가설은 입력 데이터와 출력 사이의 관계를 모델링하는 함수입니다. 이는 모델이 데이터를 기반으로 예측을 수행하는 데 사용되는 기본적인 수학적 틀을 제공합니다.
• **선형 회귀(Linear Regression)**에서는 H(x) = Wx + b와 같은 형태로 가설을 표현합니다. 여기서 W는 가중치(Weight), b는 바이어스(Bias)를 나타냅니다.
• **로지스틱 회귀(Logistic Regression)**와 같은 이진 분류 문제에서는 선형 회귀의 출력이 0과 1을 벗어날 수 있기 때문에, 시그모이드(Sigmoid) 함수를 활성 함수로 사용하여 출력을 0과 1 사이의 확률 값으로 변환합니다. 로지스틱 회귀의 가설 함수는 H(X) = 1 / (1 + e^-WTX) 형태를 가집니다.
3. 비용 함수 (Cost Function) / 손실 함수 (Loss Function)
비용 함수는 모델의 예측값이 참값(실제 값)과 얼마나 다른지 측정하는 함수입니다. 이 값이 작을수록 모델의 성능이 좋다고 판단합니다. 모델 학습의 목표는 이 비용 함수를 최소화하는 것입니다.
• 선형 회귀에서는 일반적으로 **평균 제곱 오차(Mean Squared Error, MSE)**를 비용 함수로 사용합니다. 공식은 cost(W, b) = (1/m) * Σ(H(x(i)) - y(i))^2 (또는 1/2m) 형태를 가집니다.
• 로지스틱 회귀와 같은 이진 분류 문제에서는 cost(H(x), y) = -y log(H(x)) - (1 - y) log(1 - H(x)) 형태의 비용 함수가 사용됩니다.
• 다중 분류 문제에서는 주로 크로스 엔트로피(Cross-Entropy) 손실 함수를 사용합니다. 일반적으로 분류 문제에 대해서는 MSE보다 크로스 엔트로피 함수를 사용하는 것이 학습이 더 잘 되는 것으로 알려져 있습니다.
4. 최적화 (Optimization) - 경사 하강법 (Gradient Descent)
경사 하강법은 비용 함수를 최소화하여 모델의 최적 파라미터(W, b)를 찾는 알고리즘입니다. 이는 비용 함수 그래프에서 가장 낮은 지점(최솟값)을 찾아 내려가는 과정과 유사합니다.
• 파라미터 업데이트는 W := W - α * (∂/∂W)cost(W)와 같은 공식으로 이루어집니다. 여기서 α는 **학습률(Learning Rate)**입니다.
• 학습률은 경사 하강법에서 파라미터를 업데이트하는 보폭을 결정합니다. 학습률이 너무 크면 최솟값을 지나쳐 **발산(overshooting)**할 수 있고, 너무 작으면 학습 시간이 오래 걸리거나 **지역 최솟값(local minimum)**에 갇힐 수 있습니다. 따라서 적절한 학습률을 찾는 것이 중요합니다.
• 경사 하강법 외에도 AdamOptimizer와 같은 다양한 옵티마이저들이 존재하며, 이들은 확률적 최적화를 위한 방법으로 학습 성능을 향상시키는 데 기여합니다. 예를 들어, MNIST 분류에서 Adam 옵티마이저를 사용하면 정확도가 0.9035에서 0.9783으로 크게 향상될 수 있습니다.
5. 오버피팅 (Overfitting) 및 언더피팅 (Underfitting)
모델 학습 과정에서 발생할 수 있는 주요 문제들입니다.
• **오버피팅(Overfitting)**은 모델이 트레이닝 데이터에 과도하게 최적화되어 트레이닝 데이터에서는 잘 작동하지만, 새로운 테스트 데이터에 대해서는 성능이 떨어지는 현상을 말합니다. 이는 모델의 표현력이 지나치게 강력할 경우 발생하기 쉽습니다.
• **언더피팅(Underfitting)**은 오버피팅의 반대 상황으로, 모델의 표현력이 부족하여 트레이닝 데이터조차 제대로 예측하지 못하는 상황을 의미합니다.
• 오버피팅 해결책:
    ◦ 더 많은 트레이닝 데이터를 확보하는 것이 가장 직접적인 해결책입니다.
    ◦ 피처(특징) 수를 줄이는 것도 방법이 될 수 있습니다.
    ◦ **정규화(Regularization)**는 가중치(weight) 값이 너무 커지는 것을 방지하여 모델의 복잡도를 줄여 오버피팅을 방지합니다.
    ◦ **드롭아웃(Dropout)**은 학습 시 무작위로 뉴런을 비활성화하여 오버피팅을 방지하는 기법입니다. 예를 들어, keep_prob: 0.7과 같이 설정하여 70%의 뉴런만 유지하고 30%를 비활성화하는 방식입니다. MNIST 분류에 드롭아웃을 적용하여 정확도를 0.9804로 향상시킬 수 있었습니다.
6. 데이터 전처리 (Data Preprocessing)
모델이 데이터를 효율적으로 학습할 수 있도록 데이터를 정제하고 변환하는 과정입니다.
• 표준화(Standardization): 각 피처의 평균을 0으로, 표준 편차를 1로 만들어 데이터의 스케일을 맞춥니다. 이는 경사 하강법과 같은 최적화 알고리즘의 효율적인 수렴에 도움을 줍니다.

[인공신경-ANN]
1. 인공신경망의 등장 배경 및 기본 아이디어
인공신경망은 이름에서 알 수 있듯이 인간 뇌의 생물학적 신경망(Biological Neural Networks)에서 아이디어를 얻었습니다. 인간의 뇌는 약 1000억 개의 뉴런이 서로 연결되어 병렬적으로 정보를 처리하며, 1개의 뉴런은 약 1000개의 다른 뉴런과 연결되어 약 100조 개의 연결이 존재합니다. 연구자들은 이러한 인간 뇌의 대량 병렬 처리 연산 능력에 영감을 받아 "컴퓨터도 인간의 뇌처럼 대량의 병렬 처리 연산을 수행하도록 만들면 컴퓨터도 인간이 쉽게 할 수 있는 인지 행동을 할 수 있지 않을까?"라는 아이디어로부터 초기 인공신경망 구조를 디자인했습니다.
2. 퍼셉트론(Perceptron): 인공신경망의 시초
퍼셉트론은 생물학적 뉴런을 공학적인 구조로 변형한 초기 인공신경망 모델입니다.
• 구조 및 작동 방식: 퍼셉트론은 **입력층(Input Layer)과 출력층(Output Layer)**을 가지며, 입력 데이터 $x$를 **가중치 $W$**와 곱하고 **바이어스 $b$**를 더한 후, 이 값을 **활성 함수 $\sigma$**의 입력값으로 대입하여 최종적으로 0 또는 1의 값을 출력합니다. 이는 수학적으로 **$y = \sigma(Wx + b)$**로 표현됩니다. 퍼셉트론은 **선형 이진 분류기(Linear Binary Classifier)**의 역할을 합니다.
• 한계: 초기에는 큰 기대를 받았지만, 1969년 Marvin Minsky와 Seymour Papert가 집필한 책에서 퍼셉트론이 단순한 선형 분류기에 불과하며, XOR 문제와 같이 선형 분리가 불가능한 문제를 해결할 수 없다는 사실이 수학적으로 증명되면서 인기가 급속히 사그라들었습니다.
• 의의: 그러나 퍼셉트론은 최초로 인공신경망 개념을 공학적인 구조로 구현했다는 점에서 큰 의미가 있는 모델입니다.
3. 다층 퍼셉트론(Multilayer Perceptron, MLP)과 깊은 인공신경망(DNN)
**다층 퍼셉트론(MLP)**은 오늘날 우리가 일반적으로 **인공신경망(ANN)**이라고 부르는 모델을 의미합니다.
• 구조: MLP는 **입력층, 은닉층(Hidden Layer), 출력층(Output Layer)**으로 구성됩니다. 은닉층은 데이터의 숨겨진 특징을 학습하는 역할을 합니다.
• 연산: 다층 퍼셉트론의 각 노드에서 수행하는 연산은 퍼셉트론 구조와 동일하게 $y = \sigma(Wx + b)$ 형태를 가집니다.
• 비선형 활성 함수: 퍼셉트론이 계단 함수를 활성 함수로 사용하여 이진 분류를 수행했던 것과 달리, MLP 구조에서는 모델이 비선형적인 특징을 학습할 수 있도록 비선형 활성 함수를 사용합니다.
• 깊은 인공신경망: 다층 퍼셉트론에서 은닉층을 깊게 여러 번 쌓아 올린 형태를 **깊은 인공신경망(Deep Neural Networks, DNN)**이라고 부르며, 이것이 일반적으로 **딥러닝(Deep Learning)**이라고 불리는 기법입니다. XOR 문제의 경우, 단일 퍼셉트론으로는 풀 수 없지만, 여러 개의 로지스틱 회귀 유닛(즉, MLP)을 사용하여 해결할 수 있습니다. 실제로 2개 또는 4개의 은닉층을 가진 신경망이 XOR 문제에서 1.0의 정확도를 보이는 것을 확인할 수 있습니다.
4. 인공신경망 학습의 핵심 요소
MLP와 DNN을 효과적으로 학습시키기 위한 여러 핵심 요소들이 있습니다.
4.1. 활성 함수(Activation Function)
활성 함수는 신경망의 각 노드에서 출력을 결정하며, 비선형성을 도입하여 모델이 복잡한 패턴을 학습할 수 있도록 돕습니다.
• 시그모이드(Sigmoid): 과거에 많이 사용되었던 활성 함수입니다. 그러나 기울기 소실(Vanishing Gradient) 문제로 인해 깊은 신경망에서는 학습이 느려지거나 멈출 수 있는 단점이 있습니다.
• 쌍곡탄젠트(Tanh): 시그모이드와 유사하지만 출력 범위가 -1에서 1입니다.
• ReLU(Rectified Linear Unit): 최근 딥러닝 학습에 더 적합하다고 알려져 많이 사용하는 추세입니다. 시그모이드의 기울기 소실 문제를 완화하여 학습 속도를 향상시킵니다.
4.2. 가중치 초기화(Weight Initialization)
신경망의 초기 가중치 값을 어떻게 설정하는지는 학습 성능에 매우 중요합니다. 모든 가중치를 0으로 설정하는 것은 좋지 않으며, "멍청한(stupid) 방식"으로 가중치를 초기화하는 것은 학습에 방해가 될 수 있습니다.
• Xavier 초기화(Xavier Initialization): 가중치 값이 너무 작거나 너무 크지 않도록 입력 및 출력 뉴런 수를 사용하여 초기화하는 방법입니다. MNIST 데이터셋을 사용한 실험에서, 일반적인 정규 분포 초기화(정확도 0.9455)보다 Xavier 초기화(정확도 0.9783)가 훨씬 더 높은 정확도를 보여주며 비용(cost) 또한 빠르게 줄어드는 것을 확인할 수 있습니다.
• He 초기화(He's Initialization): ReLU 활성 함수에 최적화된 초기화 방법입니다. 가중치 초기화는 여전히 활발한 연구 분야입니다.
4.3. 역전파(Backpropagation)
역전파는 인공신경망에서 가중치(W)와 바이어스(b)를 학습시키는 핵심 알고리즘입니다. 이는 퍼셉트론의 한계를 넘어서 다층 신경망을 훈련할 수 있게 한 중요한 발견입니다.
• 작동 방식: 역전파는 "연쇄 법칙(Chain Rule)"을 사용하여 비용 함수의 기울기(Gradient)를 계산합니다. 이 계산된 기울기를 사용하여 각 계층의 파라미터를 업데이트함으로써 비용 함수를 최소화합니다.
• 중요성: TensorFlow와 같은 라이브러리가 역전파 과정을 자동적으로 처리하더라도, 그 내부 동작을 이해하는 것이 신경망을 구축하고 디버깅하는 데 매우 중요합니다.
4.4. 옵티마이저(Optimizers)
비용 함수를 최소화하여 최적의 파라미터를 찾는 알고리즘을 옵티마이저라고 합니다.
• 경사 하강법(Gradient Descent): 기본적인 옵티마이저이며, 학습률(learning rate)에 따라 파라미터 업데이트의 보폭을 결정합니다.
• Adam(AdamOptimizer): "확률적 최적화를 위한 방법"으로, MNIST 분류기 성능 향상에 크게 기여했습니다. 실제로 Softmax 분류기에서 Adam 옵티마이저를 사용했을 때 정확도가 0.9035에서 0.9783으로 향상되는 예시가 제시되어 있습니다. 그 외에도 Adadelta, Adagrad, Momentum, RMSProp 등 다양한 옵티마이저들이 있습니다.
4.5. 오버피팅(Overfitting) 방지: 드롭아웃(Dropout)
오버피팅은 모델이 트레이닝 데이터에 과도하게 최적화되어, 새로운 데이터에 대한 성능이 떨어지는 현상입니다. 신경망은 표현력이 강력하여 오버피팅에 취약할 수 있습니다.
• 해결책 중 하나, 드롭아웃: 드롭아웃은 학습 시 무작위로 뉴런을 비활성화(drop out)하여 오버피팅을 방지하는 기법입니다. 트레이닝 시에는 특정 비율($0.7$ 등)의 뉴런을 유지하고, 테스트 시에는 모든 뉴런을 활성화합니다(keep_prob: 1). 드롭아웃을 적용한 깊은 인공신경망은 MNIST에서 0.9804의 높은 정확도를 달성하며 오버피팅을 효과적으로 줄일 수 있음을 보여줍니다.
5. 실습 도구 및 데이터셋
• MNIST 데이터셋: 머신러닝 학습자들이 가장 먼저 접하게 되는 데이터셋으로, "머신러닝 분야의 'Hello World'"라고 불립니다. 0~9 사이의 28x28 크기 필기체 이미지 60,000장의 트레이닝 데이터와 10,000장의 테스트 데이터로 구성되어 있습니다.
• TensorBoard: TensorFlow에서 제공하는 강력한 로깅/디버깅 도구입니다.
    ◦ TF 그래프 시각화.
    ◦ 비용, 정확도와 같은 정량적 지표 플로팅.
    ◦ 가중치 히스토그램이나 이미지와 같은 추가 데이터 표시. TensorBoard는 학습 과정을 효율적으로 모니터링하고 모델의 동작을 이해하는 데 큰 도움을 줍니다






