<numpy>
ë¡œìš°ë ˆë²¨ XOR ì‹ ê²½ë§ êµ¬í˜„ (1ì€ë‹‰ì¸µ, 1ì¶œë ¥ì¸µ, ì‹œê·¸ëª¨ì´ë“œ)
python
ë³µì‚¬
í¸ì§‘
import numpy as np

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ & ê·¸ ë„í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    return sigmoid(x) * (1 - sigmoid(x))

# ë°ì´í„° (XOR)
x_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])  # shape: (4, 2)
y_data = np.array([[0], [1], [1], [0]])     # shape: (4, 1)

# ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(42)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
W1 = np.random.randn(2, 2)   # (ì…ë ¥2 â†’ ì€ë‹‰2)
b1 = np.zeros((1, 2))        # ì€ë‹‰ì¸µ ë°”ì´ì–´ìŠ¤

W2 = np.random.randn(2, 1)   # (ì€ë‹‰2 â†’ ì¶œë ¥1)
b2 = np.zeros((1, 1))        # ì¶œë ¥ì¸µ ë°”ì´ì–´ìŠ¤

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
learning_rate = 0.1
epochs = 10000

# í•™ìŠµ ë°˜ë³µ
for epoch in range(epochs):
    # --- Forward ---
    z1 = np.dot(x_data, W1) + b1     # (4,2)
    a1 = sigmoid(z1)                 # ì€ë‹‰ì¸µ í™œì„±í™”ê°’

    z2 = np.dot(a1, W2) + b2         # (4,1)
    a2 = sigmoid(z2)                 # ì¶œë ¥ê°’

    # --- Loss (Binary Cross Entropy) ---
    loss = -np.mean(y_data * np.log(a2 + 1e-8) + (1 - y_data) * np.log(1 - a2 + 1e-8))

    # --- Backpropagation ---
    dz2 = a2 - y_data                    # (4,1)
    dW2 = np.dot(a1.T, dz2) / 4         # (2,1)
    db2 = np.sum(dz2, axis=0, keepdims=True) / 4

    dz1 = np.dot(dz2, W2.T) * sigmoid_deriv(z1)  # (4,2)
    dW1 = np.dot(x_data.T, dz1) / 4              # (2,2)
    db1 = np.sum(dz1, axis=0, keepdims=True) / 4

    # --- Gradient Descent ---
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1

    # --- ì¶œë ¥ ---
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

<tensorflow and numpy>_______________________________________________________________________________________________________________________

import tensorflow as tf
import numpy as np

# ë°ì´í„° (XOR ë¬¸ì œ)
x_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]], dtype=np.float32)  # (4, 2)
y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)  # (4, 1)

# ë³€ìˆ˜ (ê°€ì¤‘ì¹˜ & ë°”ì´ì–´ìŠ¤) ì •ì˜
W1 = tf.Variable(tf.random.normal([2, 2]))
b1 = tf.Variable(tf.zeros([2]))

W2 = tf.Variable(tf.random.normal([2, 1]))
b2 = tf.Variable(tf.zeros([1]))

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + tf.exp(-x))

# ì†ì‹¤ í•¨ìˆ˜ (Binary Cross-Entropy)
def loss_fn(y_pred, y_true):
    return tf.reduce_mean(-y_true * tf.math.log(y_pred + 1e-7) - (1 - y_true) * tf.math.log(1 - y_pred + 1e-7))

# í•™ìŠµë¥  ë° ì˜µí‹°ë§ˆì´ì €
learning_rate = 0.1
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)

# í•™ìŠµ
for epoch in range(10001):
    with tf.GradientTape() as tape:
        # Forward pass
        z1 = tf.matmul(x_data, W1) + b1      # ì€ë‹‰ì¸µ ì„ í˜• ì—°ì‚°
        a1 = sigmoid(z1)                     # ì€ë‹‰ì¸µ ì¶œë ¥

        z2 = tf.matmul(a1, W2) + b2          # ì¶œë ¥ì¸µ ì„ í˜• ì—°ì‚°
        y_pred = sigmoid(z2)                 # ìµœì¢… ì¶œë ¥

        loss = loss_fn(y_pred, y_data)

    # Gradient ê³„ì‚° ë° ì ìš©
    gradients = tape.gradient(loss, [W1, b1, W2, b2])
    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))

    # ì¶œë ¥ (1000ë§ˆë‹¤)
    if epoch % 1000 == 0:
        print(f"Epoch {epoch:5d}: Loss = {loss.numpy():.5f}")

# ì˜ˆì¸¡ ì¶œë ¥
z1 = tf.matmul(x_data, W1) + b1
a1 = sigmoid(z1)
z2 = tf.matmul(a1, W2) + b2
y_pred = sigmoid(z2)

print("\nğŸ” ì˜ˆì¸¡ ê²°ê³¼:")
print(tf.round(y_pred).numpy())

# --- í•™ìŠµ í›„ ê²°ê³¼ ---
print("\nğŸ” ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼:")
a2_final = sigmoid(np.dot(sigmoid(np.dot(x_data, W1) + b1), W2) + b2)
print(np.round(a2_final, 4))
